{
  "txt_file": "2022.01.11.475766v1.full.txt",
  "pdf_file": "2022.01.11.475766v1.full.pdf",
  "title": "1",
  "experimental_sections": [
    {
      "heading": "Materials and Methods",
      "text": "A. Machine Learning Main Procedure and Training the Machine Learning Models:\nEnzyme data from the Brenda Database was used to train the machine learning models to predict \nthe enzyme\u2019s Topt. After obtaining the data, the algorithm split the data set into a training (90% of the \ndata set) and independent test set (10% of the data set). The inputs for the model were the enzyme \nfeatures calculated from the amino acid sequence and are listed in Table 1. These inputs include amino \nacid frequency, dipeptide frequency, and optimal growth temperature (OGT). Optimal growth \ntemperature is defined as the temperature at which the host organism of the enzyme has maximum growth\nand reproduction. OGT was listed in the BRENDA database. The remaining features were calculated \nfrom the amino acid sequences through the modLAMP python library(12). Before performing a cleanup \nof the BRENDA data set, there were 2745 enzyme amino acid sequences comprising the dataset. During \nthe cleanup process, we used the Pandas library to drop duplicate rows, amino acid sequences with a \nlength less than or equal to 7, and enzymes with a Topt equal to or lower than 0 degrees Celsius. The final\ntraining data consisted of 2643 enzyme amino acid sequences listed with the experimental Topts of each \nenzyme.\nThree Machine learning models were trained to predict Topt: Random Forest, Linear Regression \nand Logistic regression. Classification Models such as logistic regression are concerned with predicting a \ndiscrete label, such as Topt >=  65 degrees C. Regression models such as Linear Regression and Random \nForest models fundamentally predict a continuous quantity such as the actual Topt value for the enzymes. \nThe three Machine Learning models were implemented using the scikit python library (11).  \nIn the training stage, first Linear Regression was trained to have a high R^2 value (correlation \nvalue) between the Predicted Topt and the actual Topt of the enzymes. However in ordinary least squares \nregression,  the R^2 values were only 0.607 for the training set and 0.33 for the test set. This showed that \nLinear regression was overfitting and not generalizing well to unseen data from the test set. Thus, we had \nto regularize Linear regression by using the lasso method. Lasso Regression is linear regression with \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprintthis version posted January 12, 2022. ; https://doi.org/10.1101/2022.01.11.475766doi: bioRxiv preprint \n8\nadded l1 regularization. This regularization adds a penalty to a model if the model is overfitting, meaning \nif it is setting the value of its weights too high. Here,Lasso regularization set the weights of the enzyme \nfeatures that were unimportant to Linear Regression to 0. Now after implementing Lasso, Linear \nregression obtained an R^2 value of 0.54 on the training set and 0.52 on the test set. \nThen a Random Forest regression model was trained which performed much better on \nminimizing error between the predicted and actual Topt. Random Forest models operate by assembling \ndifferent decision trees, then taking the mean of those decision trees and setting that as the predicted \noutput.  Random Forest attained a R^2 value of 0.9322 on the training set and 0.624 on the test set.  \nLogistic Regression was the classifier that was trained and it was measured by its accuracy in \npredicting whether an enzyme\u2019s Topt was above 65 degrees Celsius. Logistic Regression attained an \naccuracy of 92.6% on the training set and 88.3% on the test set. The graphs of these results are shown in \nFigure 6.\nB. Directed Evolution Procedure:\nThe ML-guided directed evolution algorithm generated a thousand mutants of PETase on every \niteration and screened them using Random Forest Regression. The enzyme with the highest Topt was then\nselected and mutated in the next iteration of the algorithm. This process is known as machine learning \n(ML) guided in silico directed evolution and it was used in order to steer the mutants of PETase towards \nhigher thermostability. In this type of directed evolution, machine learning is used to score and evaluate \ndifferent possible mutations of enzymes. Based on the machine learning scores, the algorithm then selects\nthe best mutant and uses it as a starting point again.\nFirst, in our ML-guided directed evolution approach, the algorithm randomly mutated the PETase\nenzyme at random positions, excluding the substrate site at positions where the PET molecule binds to the\nenzyme according to the Uniprot database (13). The algorithm did not mutate the binding site of the \nenzyme because this might decrease enzymatic activity. A thousand mutants were generated in this way. \nSecond, based on those mutations, the algorithm used Random Forest to score the corresponding mutants \nbased on their predicted Topt values. The best scoring mutant was then selected based on its optimal \ntemperature. The algorithm then repeated this mutation and selection process with the top mutant from \nthe previous iteration now acting as the main enzyme. Summed up, after performing directed evolution on\nthe mutants of PETase, the algorithm selected the best scoring mutant and performed random mutations \non it at random positions, excluding the substrate site. This process was repeated for 1000 iterations and \nyielded an enzyme with 71.38 degrees C Topt. However, 1000 iterations would lead to approximately \n1000 mutations in the original enzyme, which would create an enzyme vastly different from the original \nPETase. The original PETase enzyme has only 290 amino acids. Thus, this process was then also repeated\n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprintthis version posted January 12, 2022. ; https://doi.org/10.1101/2022.01.11.475766doi: bioRxiv preprint \n9\nfor 29 iterations in order to yield a mutant enzyme more similar to PETase rather than a completely new \nenzyme. After 29 iterations, ML-guided directed evolution produced an enzyme with a predicted Topt of \n61.3 degrees C.  \nC. External Validation of Mutant PETase Enzyme\nThe enzyme\u2019s melting temperature ranges and TM index values were generated by the online melting \ntemperature predictor by Ku et al (8). The TM values are shown in Table 4. These melting temperatures \nfurther validate that the mutant enzymes produced by the algorithm are stable and can function at their \noptimal temperatures without breaking down. These values show that the PETase and the mutant PETase \nenzymes\u2019 thermostability is between 55 and 65 degrees Celsius. The TM index for the mutated PETase \nenzyme is larger than that of the original PETase enzyme, signifying that increasing the enzyme\u2019s Topt \nalso increased the enzyme\u2019s thermostability."
    }
  ],
  "computational_sections": [
    {
      "heading": "Performing directed evolution using machine learning on the computer is known as in silico",
      "text": "directed evolution. For in silico directed evolution, instead of producing those mutants in the lab, machine\nlearning is used to score and evaluate different possible mutations of enzymes. Based on the machine \nlearning scores, the algorithm then selects the best mutant and uses it as a starting point again. Machine \nlearning (ML) guided directed evolution is beneficial because machine learning algorithms can take in \nmore data at once, iterations are faster, and the process is cheaper and less time consuming than actually \nperforming directed evolution in the lab. One challenge that exists with this method is that if the machine \nlearning algorithms written do not have a high performance, directed evolution will not achieve its \npurpose. Another challenge is that the machine learning models that make up the algorithm often perform \nbetter with classification tasks rather than predicting a continuous score. One case where ML-guided \ndirected evolution has been used successfully for enzyme engineering is in 2019, where one group of \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprintthis version posted January 12, 2022. ; https://doi.org/10.1101/2022.01.11.475766doi: bioRxiv preprint \n3\nresearchers engineered a new enzyme for stereodivergent carbon\u2013silicon bond formation, a new-to-nature\nchemical transformation (6). However, machine learning guided directed evolution has not previously \nbeen used to engineer enzymes which break down non-biodegradable plastic.\nWe hypothesize that machine learning can be used to predict an enzyme\u2019s optimal temperature, \nand machine learning can be combined with directed evolution to engineer an optimized mutant of \nPETase with a Topt greater than 60\u00b0C for more efficient breakdown of PET and nonbiodegradable \nplastic."
    },
    {
      "heading": "In this project, machine learning was used to perform in silico directed evolution on the PETase",
      "text": "enzyme to design a mutant which has predicted Topt of 70\u00b0C, nearly double that of the wild type PETase.\nThis novel enzyme has the potential to break down non-biodegradable plastic more efficiently and at a \nfaster rate than PETase by functioning at a higher optimal temperature. This enzyme is also predicted by \nexternal algorithms to have a higher thermostability than the original PETase enzyme. This approach is \nnovel because it is the first to optimize the PETase enzyme\u2019s optimal reaction temperature using a \nmachine-learning guided directed evolution approach. \nFirst, I used enzymes from the Brenda database to train the Linear Regression, Logistic \nRegression and Random Forest to predict an enzyme\u2019s Topt with high performance. Then we performed \nin silico directed evolution on PETase by generating mutant enzymes and then screening them against the\nMachine Learning models to predict the new mutant enzyme\u2019s Topt and select new mutants for further \nevolution."
    }
  ],
  "results_sections": [
    {
      "heading": "Results",
      "text": "A Machine-learning guided directed evolution algorithm was written in Python in order to \nengineer PETase for a higher thermostability. A flowchart of the approach is shown in Figures 1, 2 and 3. \nIn order to guide the directed evolution, a machine learning model was written to predict an enzyme\u2019s \noptimal reaction temperature (Topt). Three machine learning models: Random Forest, Linear Regression \nand Logistic Regression, were trained for this task using enzymes from the BRENDA database. In the \nsecond stage, my algorithm generated millions of mutants of PETase by randomly mutating different \npositions of the amino acid sequence and scored the mutants using Random Forest Regression \u2013 the ML \nmodel which performed the best \u2013 to determine which mutation would lead to the highest Topt. The \nalgorithm then reenacted this mutation and selection process with the best scoring mutants, which now \nacted as the starting point for the next round of directed evolution.\nMachine-Learning Models for Predicting Optimal Reaction Temperature\n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprintthis version posted January 12, 2022. ; https://doi.org/10.1101/2022.01.11.475766doi: bioRxiv preprint \n4\nThree machine learning models were trained to predict an enzyme\u2019s Topt. Random Forest and \nLinear Regression were used to predict the actual Topt value of the enzymes. Logistic Regression was \nused as a classification model to predict whether the enzyme had a Topt above 65 degrees Celsius. \nThe data set for the machine learning models consisted of enzymes from 11,420 organisms in \ntotal obtained from the BRENDA Database (7). Enzymes without Topt values were removed from the \ndataset. Before performing a cleanup of the data set, there were 2745 enzyme amino acid sequences \ncomprising the dataset. During the cleanup process, we dropped duplicates, amino acid sequences with a \nlength less than or equal to 7 and enzymes with a Topt equal to or lower than 0 degrees Celsius. The final \ntraining data consisted of 2643 enzyme amino acid sequences listed with the experimental Topts of each \nenzyme. The inputs for the models are the enzyme features such as molecular weight, amino acid \nfrequencies, dipeptide frequencies, and the enzyme\u2019s host organism\u2019s optimal growth temperature, as \nshown in Table 1. These enzyme features were calculated from the amino acid sequence listed from the \nBRENDA database.\nThe Random Forest and Linear Regression models were evaluated based on the R^2 value. The \nLogistic Regression model was evaluated using an accuracy score. The results are shown in Table 2. \nLasso Linear Regression obtained an R^2 value of 0.54 on the training set and 0.52 on the test set. \nRandom Forest attained a R^2 value of 0.9322 on the training set and 0.624 on the test set.  \nLogistic Regression attained an accuracy of 92.6% on the training set and 88.3% on the test set. The \ngraphs of these results are shown in Figure 6.\nFeature Ranking and Selection:\nUsing Lasso Linear Regression, we ranked the enzyme features that we used as inputs for the \nmodels and algorithms. The most important 10 features are shown in Table 5. Out of 431 features, 156 \nfeatures had non-zero coefficients and were kept by Lasso Linear Regression in the feature set. \nDirected Evolution for PETase Engineering\nThe amino acid sequence for the old original PETase enzyme is shown above in the left column \nof Table 4 with its Topt of 42 degrees Celsius. The Amino Acid sequence for the newly designed \nenzymes with 1000 and 29 iterations of directed evolution are shown in the second and third columns of \nTable 3. The enzyme generated with 1000 mutations has a predicted Topt of 71.38 degrees C.  The \nenzyme generated with 29 mutations has a predicted Topt of 61.3 degrees C.  Figure 5 shows the Topt of \nthe mutant enzyme developed after 29 iterations, increasing through the iterations.\nThe thermostability of the enzyme was measured by the predicted melting temperatures shown in \nTable 4. We used the melting temperature predictor published by Ku et al (8) to predict these melting \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprintthis version posted January 12, 2022. ; https://doi.org/10.1101/2022.01.11.475766doi: bioRxiv preprint \n5\ntemperatures.  The TM index is proportional to the actual melting temperatures of the enzymes. The TM \nIndex > 1 implies that the True Melting temperature value of the protein may exceed 65 \u25e6C (high Tm \nprotein), whereas a TM < 0 implies that the True Melting temperature value may be below 55 degrees C. \nA TM index between 0 and 1 implies that the true melting temperature is within 55 and 65 degrees \nCelsius. As seen in Table 4, the original PETase enzyme had a melting temperature in the range of 55 to \n65 degrees C and a TM index of 0.778. The mutant PETase enzyme after 1000 iterations was also \npredicted as having a melting temperature in the range of 55 to 65 degrees C and achieved a TM index of \n0.458.  The mutant PETase enzyme after 29 iterations was also predicted as having a melting temperature \nin the range of 55 to 65 degrees C and received a TM index of 0.988."
    },
    {
      "heading": "Machine Learning Results:",
      "text": "Out of the three Machine learning models trained, Random Forest was the best regression model for \ncalculating the actual Topt while Logistic Regression was the best classifier for predicting a Topt above \n65 degrees C.  The models were evaluated on a training and test set. A training set is the data used to \nbuild and train the model. A test set determines how well the model performs on data outside the training \nset. The regression models were scored based on the coefficient of determination (R^2 metric). The R2 \nscore is a metric that is used to evaluate the performance of a regression-based machine learning model. It\nmeasures the amount of variance in the predictions explained by the dataset. The classification models \nwere scored based on accuracy. Accuracy is the fraction of correct predictions over total predictions. \n       The inputs of the models and the algorithm were the enzyme features. Lasso Linear Regression was \nalso used to rank the features by their coefficients when predicting Topt. Out of the 431 enzyme features, \nonly 156 features had non-zero coefficients. Lasso Linear Regression kept these features in the feature set\nand dropped the features with the zero coefficients. The top 10 ranking features selected by Lasso Linear \nRegression are shown in Table 5 with their respective weights. The most important feature by far was \nOGT, the optimal growth temperature of the host organism of the enzyme. This makes sense because the \nTopt of an enzyme would naturally evolve to be around the range of the temperature where its host \norganism grows. For example, enzymes of thermophilic organisms would have to have a high Topt in \norder for the enzyme to function at the high temperatures these organisms thrive in.\nAs seen by its\u2019 R^2 value of 0.54 on the training set and 0.52 on the test set,  Linear Regression \nwith Lasso Regularization did not work very well on both of the datasets for predicting the Topt of the \nenzymes. However as shown by its similar R^2 value on both the training and test set, Linear Regression \ngeneralized well. With a R^2 value of 0.9322 on the training set and 0.624, Random Forest did very well \n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprintthis version posted January 12, 2022. ; https://doi.org/10.1101/2022.01.11.475766doi: bioRxiv preprint \n6\non the training set and was the best regression model for predicting Topt. Logistic Regression showed a \nhigh accuracy of 92.6 % on the training set and of 88.3% on the test set for predicting a Topt greater than \nor equal to 65 degrees Celsius. As compared to a previous research study that created a model to predict \nTopt, our algorithm performed better based on the metrics of R^2 score (9). The other model achieved a \nR^2 score of 0.51 on the test set using Random Forest and Deep Learning."
    },
    {
      "heading": "Directed Evolution Results",
      "text": "In the directed evolution stage of the algorithm, the mutant enzyme of PETase developed as an \noutput after 29 iterations is the better enzyme to test in the lab as it more closely resembles the original \nPETase enzyme as compared to the mutant enzyme developed after 1000 iterations. The mutant enzyme \nof PETase after 1000 iterations achieved a Topt of 71.38 degrees C whereas the mutant enzyme of \nPETase after 29 iterations achieved a Topt of 61.3 degrees C. As shown in Figure 4, the Topt of the \nenzyme increases at a very high rate for the first 200 iterations of Directed Evolution and then stabilizes \nand continues to rise between 65 and 70 degrees Celsius. \nThe mutant enzyme of PETase after 1000 iterations could be considered as a separate enzyme \nrather than a mutant of PETase, since its amino acid sequence differs greatly from that of the original \nPETase enzyme. In addition, the mutant PETase enzyme after 29 iterations achieved a much higher TM \nindex (0.988) than the mutant PETase enzyme after 1000 iterations (0.458). This might be due to the TM \npredictor considering the mutant PETase enzyme after 1000 iterations to be separate from the original \nPETase enzyme since their amino acid sequences vary so greatly. The mutant PETase enzyme with 29 \niterations more closely resembles the original PETase enzyme. These enzyme melting temperature values \nshow that the PETase and the mutant PETase enzymes\u2019 thermostability is between 55 and 65 degrees \nCelsius. This means that we can indeed optimize the optimal temperatures of PETase above 60 degrees \nCelsius for maximum efficiency, and the enzyme will still be stable and working at those temperatures. In\naddition, our 29-iteration mutated PETase enzyme has a higher TM index as compared to the original \nPETase enzyme, which provides external validation that our ML-guided directed evolution was \nsuccessful. This also signifies that increasing the enzyme\u2019s Topt also increased its thermostability."
    },
    {
      "heading": "Conclusions and Future Work",
      "text": "In order to avoid potential sources of error, while generating mutants, the algorithm avoided \nmutating the substrate site of the PETase enzyme in order to ensure the enzyme would continue to \nfunction. This was implemented by preserving certain positions on the amino acid sequence and making \nsure the algorithm excluded those areas while mutating. Those positions corresponded to  the substrate \nbinding positions on the enzyme.\n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprintthis version posted January 12, 2022. ; https://doi.org/10.1101/2022.01.11.475766doi: bioRxiv preprint \n7\nIn looking towards future work on this project, we plan to synthesize and test the two mutant \nenzymes this algorithm produced in the lab, both after 1000 and 29 iterations. We will also test their \noptimal temperature for breaking down PET. We will express the enzymes into bacteria and then measure\nthe bacteria\u2019s efficiency in degrading PET samples of differing sizes. For a future project, I am also \nplanning to express the best mutant enzyme in Cyanobacteria, in order to allow these photosynthetic \nbacteria to degrade the plastic in the sea and convert it into non-harmful products for the ocean and \nmarine life."
    },
    {
      "heading": "Figure 6: Graphs for Machine Learning training stage results:",
      "text": "Training Set Test Set\nLasso \nLinear \nRegressi\non\nRandom\nForest \nRegressi\non\nCaption: These graphs show the performance of the regression machine learning models, Lasso Linear \nRegression and Random Forest Regression, on the training and test set.  The correlation between the \npredicted Topt values and actual Topt values is shown through the R2 coefficient.\n(which was not certified by peer review) is the author/funder. All rights reserved. No reuse allowed without permission. \nThe copyright holder for this preprintthis version posted January 12, 2022. ; https://doi.org/10.1101/2022.01.11.475766doi: bioRxiv preprint"
    }
  ]
}